{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1ezivYmiEbR",
        "outputId": "debf70f9-fe0d-4e91-b0b5-cd54088d2e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Generated (2, 15): 1408 samples\n",
            "Generated (5, 14): 1368 samples\n",
            "Generated (8, 12): 1342 samples\n",
            "Generated (11, 10): 1328 samples\n",
            "Generated (14, 8): 1326 samples\n",
            "Generated (2, 15): 1408 samples\n",
            "Generated (5, 14): 1368 samples\n",
            "Generated (8, 12): 1342 samples\n",
            "Generated (11, 10): 1328 samples\n",
            "Generated (14, 8): 1326 samples\n",
            "\n",
            "=== Pretraining ===\n",
            "Epoch 1/50\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_4' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_3' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_2' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_1' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_4' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_3' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_2' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_1' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.9220\n",
            "[DEBUG] Layer 'rank_reduced_lstm_4' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_3' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_2' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_1' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 163ms/step - loss: 0.9212 - val_loss: 0.0133\n",
            "Epoch 2/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.3513 - val_loss: 8.2630e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2004 - val_loss: 3.4841e-05\n",
            "Epoch 4/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1828 - val_loss: 0.0071\n",
            "Epoch 5/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1173 - val_loss: 7.0096e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.1106 - val_loss: 0.0034\n",
            "Epoch 7/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1409 - val_loss: 0.0019\n",
            "Epoch 8/50\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1030 - val_loss: 5.0359e-04\n",
            "\n",
            "=== Fine-tuning ===\n",
            "Epoch 1/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.1368 - val_loss: 0.0090\n",
            "Epoch 2/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0835 - val_loss: 0.0013\n",
            "Epoch 3/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0642 - val_loss: 0.0025\n",
            "Epoch 4/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0390 - val_loss: 3.4070e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0313 - val_loss: 0.0051\n",
            "Epoch 6/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0320 - val_loss: 6.4716e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0335 - val_loss: 1.7846e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0313 - val_loss: 0.0029\n",
            "Epoch 9/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0300 - val_loss: 0.0037\n",
            "Epoch 10/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0223 - val_loss: 0.0061\n",
            "Epoch 11/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0285 - val_loss: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0199 - val_loss: 0.0060\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_4' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_3' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_2' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_1' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\u001b[1m28/42\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step   \n",
            "[DEBUG] Layer 'rank_reduced_lstm_4' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_3' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_2' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm_1' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\n",
            "[DEBUG] Layer 'rank_reduced_lstm' - Compressed: False\n",
            "[DEBUG] Using standard (uncompressed) path\n",
            "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\n",
            "--- Uncompressed model ---\n",
            "MSE: 0.0279  MAE: 0.1019  R²: 0.9756\n",
            "Avg inference: 40.029 ms/sample\n",
            "\n",
            "=== Model Compression ===\n",
            "\n",
            "[DEBUG] Compressing layer 'rank_reduced_lstm'\n",
            "Original weight shape: (31, 120), reduction_ratio=0.9\n",
            "Target reduced rank: 27\n",
            "Reconstruction error: 1.51e+00\n",
            "W_basis norm: 6.5268\n",
            "C matrix norm: 261.6378\n",
            "Compressed LSTM from (31, 120) to rank 27 (Param reduction: 10.0%)\n",
            "\n",
            "[DEBUG] Compressing layer 'rank_reduced_lstm_1'\n",
            "Original weight shape: (29, 112), reduction_ratio=0.9\n",
            "Target reduced rank: 26\n",
            "Reconstruction error: 1.22e+00\n",
            "W_basis norm: 7.2618\n",
            "C matrix norm: 7.7352\n",
            "Compressed LSTM from (29, 112) to rank 26 (Param reduction: 7.9%)\n",
            "\n",
            "[DEBUG] Compressing layer 'rank_reduced_lstm_2'\n",
            "Original weight shape: (25, 96), reduction_ratio=0.9\n",
            "Target reduced rank: 22\n",
            "Reconstruction error: 1.33e+00\n",
            "W_basis norm: 7.1183\n",
            "C matrix norm: 18.3070\n",
            "Compressed LSTM from (25, 96) to rank 22 (Param reduction: 9.3%)\n",
            "\n",
            "[DEBUG] Compressing layer 'rank_reduced_lstm_3'\n",
            "Original weight shape: (21, 80), reduction_ratio=0.9\n",
            "Target reduced rank: 18\n",
            "Reconstruction error: 1.37e+00\n",
            "W_basis norm: 6.5445\n",
            "C matrix norm: 9.8218\n",
            "Compressed LSTM from (21, 80) to rank 18 (Param reduction: 11.1%)\n",
            "\n",
            "[DEBUG] Compressing layer 'rank_reduced_lstm_4'\n",
            "Original weight shape: (17, 64), reduction_ratio=0.9\n",
            "Target reduced rank: 15\n",
            "Reconstruction error: 9.25e-01\n",
            "W_basis norm: 5.8533\n",
            "C matrix norm: 8.9889\n",
            "Compressed LSTM from (17, 64) to rank 15 (Param reduction: 9.0%)\n",
            "\n",
            "--- Compressed model ---\n",
            "MSE: 0.0279  MAE: 0.1019  R²: 0.9756\n",
            "Avg inference: 34.133 ms/sample\n",
            "\n",
            "Speed-up: 1.17× faster after compression\n",
            "ΔMSE: +0.00e+00, ΔMAE: +0.00e+00\n",
            "\n",
            "=== Verifying Compression Status ===\n",
            "Layer rank_reduced_lstm: compressed=True, trainable=False\n",
            "Layer rank_reduced_lstm_1: compressed=True, trainable=False\n",
            "Layer rank_reduced_lstm_2: compressed=True, trainable=False\n",
            "Layer rank_reduced_lstm_3: compressed=True, trainable=False\n",
            "Layer rank_reduced_lstm_4: compressed=True, trainable=False\n",
            "\n",
            "Avg inference time: 88.491 ms per sample\n",
            "≈ 5899.43 μs per timestep (sequence length = 15)\n",
            "\\n✅ Total run time: 146.31s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"Complete LSTM-ENSEMBLE Implementation with Rank Reduction\"\"\"\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Dense, Input, Concatenate, Multiply, Lambda, Softmax\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.signal import savgol_filter, welch\n",
        "import tensorflow as tf\n",
        "from scipy.linalg import svd\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ================== UTILITY FUNCTIONS ==================\n",
        "def load_data(file_path):\n",
        "    \"\"\"Load acceleration data from file\"\"\"\n",
        "    data = pd.read_csv(file_path, sep='\\s+', header=None, skiprows=1)\n",
        "    return data[1].values\n",
        "\n",
        "def create_sequences(data, delay_params):\n",
        "    \"\"\"Generate multi-rate sequences\"\"\"\n",
        "    sequences = {}\n",
        "    for tau, d in delay_params:\n",
        "        X, y = [], []\n",
        "        for i in range(len(data) - d*tau - 1):\n",
        "            X.append(data[i:i+d*tau:tau])\n",
        "            y.append(data[i + d*tau])\n",
        "        X = np.array(X).reshape(-1, d, 1)\n",
        "        sequences[(tau, d)] = (X, np.array(y))\n",
        "        print(f\"Generated {(tau,d)}: {X.shape[0]} samples\")\n",
        "    return sequences\n",
        "\n",
        "def align_sequences(sequences_dict, delay_params):\n",
        "    \"\"\"Ensure equal sample counts across all branches\"\"\"\n",
        "    min_samples = min(sequences_dict[(tau, d)][0].shape[0] for (tau, d) in delay_params)\n",
        "    X = [sequences_dict[(tau, d)][0][:min_samples] for (tau, d) in delay_params]\n",
        "    y = sequences_dict[delay_params[0]][1][:min_samples]\n",
        "    return X, y\n",
        "class RankReducedLSTM(Layer):\n",
        "\n",
        "    def __init__(self, units, reduction_ratio=0.8, return_sequences=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "        self.return_sequences = return_sequences\n",
        "        self.state_size = [units, units]\n",
        "        self.output_size = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        total_dim = input_dim + self.units\n",
        "\n",
        "        # Original weight matrix\n",
        "        self.W = self.add_weight(\n",
        "            shape=(total_dim, 4 * self.units),\n",
        "            name='kernel',\n",
        "            initializer='glorot_uniform')\n",
        "\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(4 * self.units,),\n",
        "            name='bias',\n",
        "            initializer='zeros')\n",
        "\n",
        "        # Compression matrices\n",
        "        self.W_basis = None\n",
        "        self.C = None\n",
        "        self.compressed = False\n",
        "\n",
        "    def call(self, inputs, initial_state=None):\n",
        "\n",
        "        # DEBUG LINE\n",
        "        print(f\"\\n[DEBUG] Layer '{self.name}' - Compressed: {self.compressed}\")\n",
        "        if self.compressed:\n",
        "            print(f\"[DEBUG] Using compressed path (rank={self.reduced_rank})\")\n",
        "        else:\n",
        "            print(f\"[DEBUG] Using standard (uncompressed) path\")\n",
        "        # Initialize states\n",
        "        if initial_state is None:\n",
        "            batch_size = tf.shape(inputs)[0]\n",
        "            h_prev = tf.zeros((batch_size, self.units), dtype=inputs.dtype)\n",
        "            c_prev = tf.zeros((batch_size, self.units), dtype=inputs.dtype)\n",
        "        else:\n",
        "            h_prev, c_prev = initial_state\n",
        "\n",
        "        # Prepare inputs - ensure 2D shape [batch_size, features]\n",
        "        if len(inputs.shape) == 3:\n",
        "            inputs = inputs[:, -1, :]  # Take last timestep if sequence\n",
        "\n",
        "        # Concatenate input and hidden state\n",
        "        z = tf.concat([inputs, h_prev], axis=-1)\n",
        "\n",
        "        if self.compressed:\n",
        "            # Compressed forward pass\n",
        "            r = tf.shape(self.W_basis)[0]  # reduced rank\n",
        "            z1 = z[:, :r]  # basis components\n",
        "            z2 = z[:, r:]  # dependent components\n",
        "\n",
        "            # Project dependent components\n",
        "            z2_coef = tf.matmul(z2, self.C)\n",
        "\n",
        "            # Combine and project through basis\n",
        "            z_reduced = z1 + z2_coef\n",
        "            z_out = tf.matmul(z_reduced, self.W_basis) + self.bias\n",
        "        else:\n",
        "            # Standard forward pass\n",
        "            z_out = tf.matmul(z, self.W) + self.bias\n",
        "\n",
        "        # Gates computation\n",
        "        i, f, c, o = tf.split(z_out, 4, axis=-1)\n",
        "\n",
        "        # LSTM operations\n",
        "        c_new = tf.sigmoid(f) * c_prev + tf.sigmoid(i) * tf.tanh(c)\n",
        "        h_new = tf.sigmoid(o) * tf.tanh(c_new)\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return tf.expand_dims(h_new, axis=1), [h_new, c_new]\n",
        "        return h_new, [h_new, c_new]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_sequences:\n",
        "            return (input_shape[0], 1, self.units)\n",
        "        return (input_shape[0], self.units)\n",
        "\n",
        "    def compress(self):\n",
        "        \"\"\"Apply verified rank reduction\"\"\"\n",
        "        W = self.W.numpy()\n",
        "        m, n = W.shape\n",
        "        # Calculate actual reduced rank\n",
        "        self.reduced_rank = max(1, int(m * self.reduction_ratio))\n",
        "\n",
        "\n",
        "        #DEBUG LINE\n",
        "        print(f\"\\n[DEBUG] Compressing layer '{self.name}'\")\n",
        "        print(f\"Original weight shape: {W.shape}, reduction_ratio={self.reduction_ratio}\")\n",
        "        print(f\"Target reduced rank: {self.reduced_rank}\")\n",
        "        # SVD decomposition\n",
        "        U, s, Vh = svd(W, full_matrices=False)\n",
        "\n",
        "        # Truncated matrices\n",
        "        Ur = U[:, :self.reduced_rank]\n",
        "        Sr = np.diag(s[:self.reduced_rank])\n",
        "        Vhr = Vh[:self.reduced_rank, :]\n",
        "\n",
        "        # Low-rank approximation\n",
        "        W_r = Ur @ Sr @ Vhr\n",
        "\n",
        "        # RECONSTRUCTION ERROR CHECK\n",
        "        reconstruction_error = np.linalg.norm(W - W_r)\n",
        "        print(f\"Reconstruction error: {reconstruction_error:.2e}\")\n",
        "\n",
        "        # Split into basis and dependent rows\n",
        "        W_basis = W_r[:self.reduced_rank, :]\n",
        "        W_dep = W_r[self.reduced_rank:, :]\n",
        "\n",
        "        # Compute projection matrix\n",
        "        C = W_dep @ np.linalg.pinv(W_basis)\n",
        "\n",
        "      #  NORM CHECK\n",
        "        print(f\"W_basis norm: {np.linalg.norm(W_basis):.4f}\")\n",
        "        print(f\"C matrix norm: {np.linalg.norm(C):.4f}\")\n",
        "\n",
        "        # Store as TF variables\n",
        "        self.W_basis = tf.Variable(W_basis, dtype=tf.float32, trainable=False)\n",
        "        self.C = tf.Variable(C, dtype=tf.float32, trainable=False)\n",
        "        self.compressed = True\n",
        "        self.trainable = False\n",
        "\n",
        "        # Calculate compression statistics\n",
        "        original_params = m * n\n",
        "        compressed_params = self.reduced_rank * (n + m - self.reduced_rank)\n",
        "        compression_ratio = compressed_params / original_params\n",
        "\n",
        "        print(f\"Compressed LSTM from {W.shape} to rank {self.reduced_rank} \"\n",
        "              f\"(Param reduction: {1-compression_ratio:.1%})\")\n",
        "\n",
        "# ================== MAIN PIPELINE ==================\n",
        "def main():\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    # === DATA LOADING & PREPROCESSING ===\n",
        "    data_pretrain = load_data('/content/drive/MyDrive/acc_base_a0.txt')\n",
        "    data_finetune = load_data('/content/drive/MyDrive/acc_base_a38.txt')\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    data_pretrain = scaler.fit_transform(data_pretrain.reshape(-1, 1)).flatten()\n",
        "    data_finetune = scaler.transform(data_finetune.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # === MULTI-RATE SAMPLING ===\n",
        "    delay_params = [(2,15), (5,14), (8,12), (11,10), (14,8)]\n",
        "    pretrain_sequences = create_sequences(data_pretrain, delay_params)\n",
        "    finetune_sequences = create_sequences(data_finetune, delay_params)\n",
        "\n",
        "    # === DATA ALIGNMENT ===\n",
        "    X_pretrain, y_pretrain = align_sequences(pretrain_sequences, delay_params)\n",
        "    X_finetune, y_finetune = align_sequences(finetune_sequences, delay_params)\n",
        "\n",
        "    # === MODEL ARCHITECTURE ===\n",
        "    COMMON_DIM = 32\n",
        "    inputs = [Input(shape=(d, 1)) for (tau, d) in delay_params]\n",
        "\n",
        "    # LSTM branches with projection and rank reduction\n",
        "    lstm_outputs = []\n",
        "    for inp, (tau, d) in zip(inputs, delay_params):\n",
        "        lstm_layer = RankReducedLSTM(\n",
        "            units=2*d,\n",
        "            reduction_ratio=0.9,  # 20% reduction\n",
        "            return_sequences=False\n",
        "        )\n",
        "        x = lstm_layer(inp)\n",
        "        x = Dense(COMMON_DIM, activation='tanh')(x)\n",
        "        lstm_outputs.append(x)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attention = Dense(len(delay_params), activation='tanh')(Concatenate()(lstm_outputs))\n",
        "    #attention_weights = Softmax(axis=1)(attention)\n",
        "    #attention_weights = Lambda(lambda x: tf.expand_dims(x, -1))(attention_weights)\n",
        "    # --- Attention weights (batch, branches) → (batch, branches, 1)\n",
        "    attention_weights = Softmax(axis=1)(attention)\n",
        "    attention_weights = Lambda(\n",
        "       lambda x, _tf=tf:_tf.expand_dims(x, axis=-1),\n",
        "       output_shape=lambda s: (s[0], s[1], 1)\n",
        "      )(attention_weights)\n",
        "\n",
        "# --- Expand each branch (batch, COMMON_DIM) → (batch, 1, COMMON_DIM)\n",
        "    expanded = [\n",
        "      Lambda(\n",
        "        lambda x , _tf=tf:_tf.expand_dims(x, axis=1),\n",
        "        output_shape=lambda s: (s[0], 1, s[1])\n",
        "      )(o)\n",
        "       for o in lstm_outputs\n",
        "       ]\n",
        "\n",
        "# --- Multiply & sum\n",
        "    concatenated = Concatenate(axis=1)(expanded)\n",
        "    weighted_output = Multiply()([concatenated, attention_weights])\n",
        "\n",
        "# --- Sum over the branches dimension\n",
        "    summed = Lambda(\n",
        "    lambda x, _tf=tf: _tf.reduce_sum(x, axis=1),\n",
        "    output_shape=lambda s: (s[0], s[2])\n",
        "    )    (weighted_output)\n",
        "\n",
        "# --- Final prediction\n",
        "    output = Dense(1)(summed)\n",
        "\n",
        "\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.006), loss='mse')\n",
        "\n",
        "    # Submodel for attention weights extraction\n",
        "    attention_model = Model(inputs=model.inputs, outputs=attention_weights)\n",
        "\n",
        "    # === TRAINING ===\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    print(\"\\n=== Pretraining ===\")\n",
        "    pretrain_history = model.fit(\n",
        "        X_pretrain, y_pretrain,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Fine-tuning ===\")\n",
        "    finetune_history = model.fit(\n",
        "        X_finetune, y_finetune,\n",
        "        epochs=20,\n",
        "        batch_size=32,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "       # === ORIGINAL (UNCOMPRESSED) EVALUATION ===\n",
        "    y_full = model.predict(X_finetune).flatten()\n",
        "    #y_full_smooth = savgol_filter(y_full, window_length=5, polyorder=2)\n",
        "    # === UNCOMPRESSED EVALUATION & TIMING ===\n",
        "    print(\"\\n--- Uncompressed model ---\")\n",
        "    mse_un = mean_squared_error(y_finetune, y_full)\n",
        "    mae_un = mean_absolute_error( y_finetune, y_full)\n",
        "    r2_un  = r2_score(          y_finetune, y_full)\n",
        "\n",
        "    # Batch-mode timing\n",
        "    t0 = time.perf_counter()\n",
        "    _  = model.predict(X_finetune, verbose=0)\n",
        "    t1 = time.perf_counter()\n",
        "    ms_per_sample_un = (t1 - t0) * 1e3 / len(X_finetune)\n",
        "\n",
        "    print(f\"MSE: {mse_un:.4f}  MAE: {mae_un:.4f}  R²: {r2_un:.4f}\")\n",
        "    print(f\"Avg inference: {ms_per_sample_un:.3f} ms/sample\")\n",
        "\n",
        "   # === MODEL COMPRESSION ===\n",
        "    print(\"\\n=== Model Compression ===\")\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, RankReducedLSTM):\n",
        "            layer.compress()\n",
        "        #  POST-COMPRESSION CHECK\n",
        "        #print(f\"[POST-COMPRESS] Layer '{layer.name}' compressed={layer.compressed}\")\n",
        "        #print(f\"  W_basis shape: {layer.W_basis.shape}\")\n",
        "        #print(f\"  C shape: {layer.C.shape}\\n\")\n",
        "    # === COMPRESSED EVALUATION & TIMING ===\n",
        "    print(\"\\n--- Compressed model ---\")\n",
        "    y_cp = model.predict(X_finetune, verbose=0).flatten()\n",
        "    mse_cp = mean_squared_error(y_finetune, y_cp)\n",
        "    mae_cp = mean_absolute_error( y_finetune, y_cp)\n",
        "    r2_cp  = r2_score(          y_finetune, y_cp)\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    _  = model.predict(X_finetune, verbose=0)\n",
        "    t1 = time.perf_counter()\n",
        "    ms_per_sample_cp = (t1 - t0) * 1e3 / len(X_finetune)\n",
        "\n",
        "    print(f\"MSE: {mse_cp:.4f}  MAE: {mae_cp:.4f}  R²: {r2_cp:.4f}\")\n",
        "    print(f\"Avg inference: {ms_per_sample_cp:.3f} ms/sample\")\n",
        "\n",
        "    # === SPEED-UP & ERROR CHANGE ===\n",
        "    speedup = ms_per_sample_un / ms_per_sample_cp\n",
        "    print(f\"\\nSpeed-up: {speedup:.2f}× faster after compression\")\n",
        "    print(f\"ΔMSE: {mse_cp - mse_un:+.2e}, ΔMAE: {mae_cp - mae_un:+.2e}\")\n",
        "\n",
        "    print(\"\\n=== Verifying Compression Status ===\")\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, RankReducedLSTM):\n",
        "           print(f\"Layer {layer.name}: compressed={layer.compressed}, trainable={layer.trainable}\")\n",
        "\n",
        "\n",
        "\n",
        "    # === TIMING BENCHMARK (per‐sample & per‐timestep) ===\n",
        "    # Warm‐up\n",
        "     = model.predict([x[:1] for x in X_finetune], verbose=0)\n",
        "     # === TIMING BENCHMARK(UPDATED TO USE compressed_model)\n",
        "    times = []\n",
        "    for i in range(len(X_finetune[0])):\n",
        "       sample = [x[i:i+1] for x in X_finetune]\n",
        "       t0 = time.perf_counter()\n",
        "       model.predict(sample, verbose=0)  # Changed from model to compressed_model\n",
        "       t1 = time.perf_counter()\n",
        "       times.append((t1 - t0) * 1e3)\n",
        "\n",
        "    mean_ms = np.mean(times)\n",
        "    seq_len = X_finetune[0].shape[1]\n",
        "    per_step_us = mean_ms * 1e3 / seq_len  # convert ms→μs then divide\n",
        "\n",
        "    print(f\"\\nAvg inference time: {mean_ms:.3f} ms per sample\")\n",
        "    print(f\"≈ {per_step_us:.2f} μs per timestep (sequence length = {seq_len})\")\n",
        "    print(f\"\\\\n✅ Total run time: {time.time() - total_start_time:.2f}s\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}